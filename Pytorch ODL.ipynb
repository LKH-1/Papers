{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydataset(data_utils.Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data = pd.read_csv(file_path, nrows = 10000)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data.iloc[index, 1:].values.astype(np.float32)\n",
    "        Y = self.data.iloc[index, 0].astype(np.float32)\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = mydataset('HIGGS.csv')\n",
    "train_loader = data_utils.DataLoader(training, batch_size=1, shuffle=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_dims (batch_size, input_dim, hidden_layers, output_dims) \n",
    "layer_dims = (10000, 28, 15, 15, 1)\n",
    "#device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(layer_dims[1], layer_dims[2])\n",
    "        self.linear1_out = nn.Linear(layer_dims[2], 1)\n",
    "        self.linear2 = nn.Linear(layer_dims[2], layer_dims[3])\n",
    "        self.linear2_out = nn.Linear(layer_dims[3], 1)\n",
    "        self.linear3 = nn.Linear(layer_dims[3], layer_dims[4])\n",
    "        self.linear3_out = nn.Linear(layer_dims[4], 1)\n",
    "        #self.linear4 = nn.Linear(layer_dims[4], layer_dims[5])\n",
    "        #self.linear4_out = nn.Linear(layer_dims[5], 1)\n",
    "        #self.linear5 = nn.Linear(layer_dims[5], layer_dims[6])\n",
    "        #self.linear5_out = nn.Linear(layer_dims[6], 1)\n",
    "        #self.linear6 = nn.Linear(layer_dims[6], layer_dims[7])\n",
    "        #self.linear6_out = nn.Linear(layer_dims[7], 1)\n",
    "        #self.linear7 = nn.Linear(layer_dims[7], layer_dims[8])\n",
    "        #self.linear7_out = nn.Linear(layer_dims[8], 1)\n",
    "        #self.linear8 = nn.Linear(layer_dims[8], layer_dims[9])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "\n",
    "        \n",
    "        layer1 = self.linear1(x)\n",
    "        layer1_act = self.relu(layer1)\n",
    "        layer1_out = self.sigmoid(self.linear1_out(layer1_act)) \n",
    "        \n",
    "        layer2 = self.linear2(layer1_act)\n",
    "        layer2_act = self.relu(layer2) \n",
    "        layer2_out = self.sigmoid(self.linear2_out(layer2_act))\n",
    "        \n",
    "        layer3 = self.linear3(layer2_act)\n",
    "        layer3_act = self.relu(layer3) \n",
    "        layer3_out = self.sigmoid(self.linear3_out(layer3_act))\n",
    "        \n",
    "        #layer4 = self.linear4(layer3_act)\n",
    "        #layer4_act = self.relu(layer4) \n",
    "        #layer4_out = self.sigmoid(self.linear4_out(layer4_act))\n",
    "        \n",
    "        #layer5 = self.linear5(layer4_act)\n",
    "        #layer5_act = self.relu(layer5) \n",
    "        #layer5_out = self.sigmoid(self.linear5_out(layer5_act))\n",
    "        \n",
    "        #layer6 = self.linear6(layer5_act)\n",
    "        #layer6_act = self.relu(layer6) \n",
    "        #layer6_out = self.sigmoid(self.linear6_out(layer6_act))\n",
    "        \n",
    "        #layer7 = self.linear7(layer6_act)\n",
    "        #layer7_act = self.relu(layer7) \n",
    "        #layer7_out = self.sigmoid(self.linear7_out(layer7_act))\n",
    "        \n",
    "        #layer8 = self.linear8(layer7_act)\n",
    "        #layer8_out = self.sigmoid(layer8)\n",
    "        \n",
    "        return layer1_out,layer2_out, layer3_out                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linear1): Linear(in_features=28, out_features=15, bias=True)\n",
      "  (linear1_out): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (linear2): Linear(in_features=15, out_features=15, bias=True)\n",
      "  (linear2_out): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (linear3): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (linear3_out): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net(layer_dims)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr= .01)\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=28, out_features=15, bias=True)\n",
       "  (linear1_out): Linear(in_features=15, out_features=1, bias=True)\n",
       "  (linear2): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (linear2_out): Linear(in_features=15, out_features=1, bias=True)\n",
       "  (linear3): Linear(in_features=15, out_features=1, bias=True)\n",
       "  (linear3_out): Linear(in_features=1, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mrugank_pc\\Softwares\\Python\\Anaconda3_5.1.0\\lib\\site-packages\\torch\\nn\\functional.py:2016: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Total_loss 0.6372918486595154 Loss1: 0.6380852460861206 Loss2: 0.6369341015815735 Loss3: 0.6368561387062073\n",
      "alpha: [0.33117426606675937, 0.3343062732597462, 0.33451946067349436]\n",
      "200 Total_loss 0.6658570766448975 Loss1: 0.6658321619033813 Loss2: 0.6658939719200134 Loss3: 0.6658475995063782\n",
      "alpha: [0.35352915473971386, 0.3194472408494711, 0.3270236044108151]\n",
      "400 Total_loss 0.6088424324989319 Loss1: 0.6087393760681152 Loss2: 0.6098716259002686 Loss3: 0.6080480813980103\n",
      "alpha: [0.35395778086121693, 0.29977656974607825, 0.3462656493927049]\n",
      "600 Total_loss 0.7815289497375488 Loss1: 0.782187283039093 Loss2: 0.7787351012229919 Loss3: 0.7831317186355591\n",
      "alpha: [0.36130134974250544, 0.28865911273656925, 0.3500395375209253]\n",
      "800 Total_loss 0.6400877237319946 Loss1: 0.6399809718132019 Loss2: 0.640520453453064 Loss3: 0.6398199200630188\n",
      "alpha: [0.35847697792235916, 0.29814787088910943, 0.3433751511885314]\n",
      "1000 Total_loss 0.8029677867889404 Loss1: 0.8032190799713135 Loss2: 0.801891028881073 Loss3: 0.8035353422164917\n",
      "alpha: [0.3704562094222059, 0.27560325439497513, 0.3539405361828188]\n",
      "1200 Total_loss 0.606968879699707 Loss1: 0.6070947051048279 Loss2: 0.606791615486145 Loss3: 0.6069996356964111\n",
      "alpha: [0.35240674191054105, 0.31148456388663415, 0.3361086942028248]\n",
      "1400 Total_loss 0.585192084312439 Loss1: 0.5866215825080872 Loss2: 0.5844414830207825 Loss3: 0.5845673680305481\n",
      "alpha: [0.3218481407004387, 0.3250857353012883, 0.353066123998273]\n",
      "1600 Total_loss 0.6397281885147095 Loss1: 0.6316463351249695 Loss2: 0.6435648202896118 Loss3: 0.6429688334465027\n",
      "alpha: [0.3058696426855284, 0.3330203686814447, 0.3611099886330268]\n",
      "1800 Total_loss 0.8061426877975464 Loss1: 0.8059701919555664 Loss2: 0.806242048740387 Loss3: 0.8062228560447693\n",
      "alpha: [0.3434063875214752, 0.31359001162958805, 0.3430036008489367]\n",
      "2000 Total_loss 0.7215690612792969 Loss1: 0.7213732600212097 Loss2: 0.7217302918434143 Loss3: 0.7216237783432007\n",
      "alpha: [0.3516025935999482, 0.3076122187306739, 0.34078518766937793]\n",
      "2200 Total_loss 0.6614460945129395 Loss1: 0.6612606048583984 Loss2: 0.6617257595062256 Loss3: 0.661379337310791\n",
      "alpha: [0.34557233607302545, 0.3088540841292033, 0.34557357979777126]\n",
      "2400 Total_loss 0.6345242857933044 Loss1: 0.6344945430755615 Loss2: 0.6345428228378296 Loss3: 0.634536862373352\n",
      "alpha: [0.3433523253807488, 0.3102369947099526, 0.3464106799092987]\n",
      "2600 Total_loss 0.6529971957206726 Loss1: 0.6537265181541443 Loss2: 0.6534584760665894 Loss3: 0.6517061591148376\n",
      "alpha: [0.3587300317041393, 0.3209147878959567, 0.32035518039990396]\n",
      "2800 Total_loss 0.7532639503479004 Loss1: 0.7526824474334717 Loss2: 0.7503225207328796 Loss3: 0.7559447288513184\n",
      "alpha: [0.33914817503698386, 0.28170897076262785, 0.3791428542003883]\n",
      "3000 Total_loss 0.6168564558029175 Loss1: 0.6185373067855835 Loss2: 0.6234269738197327 Loss3: 0.6114240884780884\n",
      "alpha: [0.33105341287728873, 0.2547081946887268, 0.4142383924339844]\n",
      "3200 Total_loss 0.7563804388046265 Loss1: 0.756190836429596 Loss2: 0.7558480501174927 Loss3: 0.7569411993026733\n",
      "alpha: [0.3356359473755952, 0.28437372583191695, 0.37999032679248784]\n",
      "3400 Total_loss 0.6764840483665466 Loss1: 0.6776851415634155 Loss2: 0.6792203783988953 Loss3: 0.6736736297607422\n",
      "alpha: [0.32681285178261393, 0.26846818100028996, 0.4047189672170961]\n",
      "3600 Total_loss 0.63048255443573 Loss1: 0.6311028003692627 Loss2: 0.632103681564331 Loss3: 0.6289001107215881\n",
      "alpha: [0.3273423669397518, 0.2670976931781516, 0.40555993988209654]\n",
      "3800 Total_loss 0.7606987953186035 Loss1: 0.7612735033035278 Loss2: 0.7622530460357666 Loss3: 0.7591469287872314\n",
      "alpha: [0.3297049584132153, 0.27208732199697966, 0.39820771958980494]\n",
      "4000 Total_loss 0.60419762134552 Loss1: 0.604200541973114 Loss2: 0.6043022871017456 Loss3: 0.6041102409362793\n",
      "alpha: [0.3340985470626134, 0.29615451436797635, 0.36974693856941027]\n",
      "4200 Total_loss 0.6632817387580872 Loss1: 0.6636145710945129 Loss2: 0.6630662083625793 Loss3: 0.6631609797477722\n",
      "alpha: [0.32508576592515687, 0.2925843447297581, 0.382329889345085]\n",
      "4400 Total_loss 0.6329599618911743 Loss1: 0.6326993107795715 Loss2: 0.6336454749107361 Loss3: 0.63263338804245\n",
      "alpha: [0.32912382315879807, 0.2992737364600487, 0.3716024403811533]\n",
      "4600 Total_loss 0.6414101719856262 Loss1: 0.6414085626602173 Loss2: 0.6415095329284668 Loss3: 0.641338050365448\n",
      "alpha: [0.33227159809610324, 0.2821812564761712, 0.3855471454277256]\n",
      "4800 Total_loss 0.6688121557235718 Loss1: 0.6687904000282288 Loss2: 0.6688681244850159 Loss3: 0.6687878370285034\n",
      "alpha: [0.3319711147176762, 0.2903252928294201, 0.37770359245290375]\n",
      "5000 Total_loss 0.7299196720123291 Loss1: 0.7368136048316956 Loss2: 0.7215314507484436 Loss3: 0.7328097820281982\n",
      "alpha: [0.28389864094225586, 0.3594743070471228, 0.35662705201062134]\n",
      "5200 Total_loss 0.6851509809494019 Loss1: 0.6838734149932861 Loss2: 0.6862653493881226 Loss3: 0.6851937770843506\n",
      "alpha: [0.2995783888600837, 0.3257254400283838, 0.3746961711115325]\n",
      "5400 Total_loss 0.654147744178772 Loss1: 0.6515206694602966 Loss2: 0.6591704487800598 Loss3: 0.651244580745697\n",
      "alpha: [0.2996811481948445, 0.35361832578258084, 0.3467005260225746]\n",
      "5600 Total_loss 0.6492854952812195 Loss1: 0.6492229700088501 Loss2: 0.6494261622428894 Loss3: 0.6492167115211487\n",
      "alpha: [0.3167828710682703, 0.3166944211227356, 0.366522707808994]\n",
      "5800 Total_loss 0.7126293182373047 Loss1: 0.713285505771637 Loss2: 0.7122517824172974 Loss3: 0.7123820781707764\n",
      "alpha: [0.3173898487871612, 0.3183397915303831, 0.3642703596824557]\n",
      "6000 Total_loss 0.750112771987915 Loss1: 0.7500751614570618 Loss2: 0.7505963444709778 Loss3: 0.7496412992477417\n",
      "alpha: [0.31543174043022504, 0.34864105175248744, 0.3359272078172876]\n",
      "6200 Total_loss 0.6332377195358276 Loss1: 0.6332342624664307 Loss2: 0.6343703269958496 Loss3: 0.6323172450065613\n",
      "alpha: [0.3183567324324741, 0.3044359161854334, 0.3772073513820925]\n",
      "6400 Total_loss 0.7657712697982788 Loss1: 0.7657564878463745 Loss2: 0.7659671306610107 Loss3: 0.7656176686286926\n",
      "alpha: [0.3181629043496996, 0.31141001683605907, 0.37042707881424125]\n",
      "6600 Total_loss 0.6136128306388855 Loss1: 0.6150025725364685 Loss2: 0.6126325726509094 Loss3: 0.6133852005004883\n",
      "alpha: [0.294886213788688, 0.33641569358002543, 0.3686980926312865]\n",
      "6800 Total_loss 0.651343047618866 Loss1: 0.65122389793396 Loss2: 0.6513919234275818 Loss3: 0.6514023542404175\n",
      "alpha: [0.3155313945998523, 0.31983576939904407, 0.3646328360011036]\n",
      "7000 Total_loss 0.7562045454978943 Loss1: 0.7558343410491943 Loss2: 0.7563397884368896 Loss3: 0.756382405757904\n",
      "alpha: [0.30115122698664065, 0.3259553668649036, 0.37289340614845584]\n",
      "7200 Total_loss 0.621312141418457 Loss1: 0.6212877631187439 Loss2: 0.6213172674179077 Loss3: 0.6213281750679016\n",
      "alpha: [0.31245002589785115, 0.3210372899948602, 0.36651268410728866]\n",
      "7400 Total_loss 0.6833804249763489 Loss1: 0.6830623149871826 Loss2: 0.6843662858009338 Loss3: 0.6827297806739807\n",
      "alpha: [0.31072528508700265, 0.3325508907325479, 0.35672382418044946]\n",
      "7600 Total_loss 0.699482262134552 Loss1: 0.6991321444511414 Loss2: 0.7002429962158203 Loss3: 0.6990548372268677\n",
      "alpha: [0.30738941362123917, 0.3375742437932898, 0.355036342585471]\n",
      "7800 Total_loss 0.6706893444061279 Loss1: 0.6707552671432495 Loss2: 0.6708813905715942 Loss3: 0.6704756021499634\n",
      "alpha: [0.3092564034293826, 0.3117394206788967, 0.37900417589172075]\n",
      "8000 Total_loss 0.7013883590698242 Loss1: 0.7014084458351135 Loss2: 0.7023921608924866 Loss3: 0.7005747556686401\n",
      "alpha: [0.3126377985002859, 0.3025793509859473, 0.3847828505137668]\n",
      "8200 Total_loss 0.6459512710571289 Loss1: 0.6458096504211426 Loss2: 0.6455440521240234 Loss3: 0.6464457511901855\n",
      "alpha: [0.3161689625747827, 0.32718405514447135, 0.35664698228074593]\n",
      "8400 Total_loss 0.5482442378997803 Loss1: 0.546281099319458 Loss2: 0.5456737279891968 Loss3: 0.5538172125816345\n",
      "alpha: [0.34824209797989647, 0.3640029396598688, 0.2877549623602348]\n",
      "8600 Total_loss 0.8552202582359314 Loss1: 0.855337381362915 Loss2: 0.8553280234336853 Loss3: 0.8549394607543945\n",
      "alpha: [0.35307583229971706, 0.3589799741769698, 0.2879441935233132]\n",
      "8800 Total_loss 0.5578525066375732 Loss1: 0.5579100847244263 Loss2: 0.5579416155815125 Loss3: 0.5576987266540527\n",
      "alpha: [0.332283508472682, 0.34218841673685935, 0.32552807479045864]\n",
      "9000 Total_loss 0.6928943991661072 Loss1: 0.6940253973007202 Loss2: 0.6974582672119141 Loss3: 0.688327968120575\n",
      "alpha: [0.31870768488001416, 0.2995014352620391, 0.3817908798579467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9200 Total_loss 0.664607048034668 Loss1: 0.6653962135314941 Loss2: 0.6672605276107788 Loss3: 0.6619144082069397\n",
      "alpha: [0.31674742010772644, 0.2955907117413913, 0.38766186815088227]\n",
      "9400 Total_loss 0.7344732880592346 Loss1: 0.7348304390907288 Loss2: 0.7354527711868286 Loss3: 0.7334564328193665\n",
      "alpha: [0.31393001975349716, 0.29145410160250806, 0.39461587864399483]\n",
      "9600 Total_loss 0.6147271394729614 Loss1: 0.6149125695228577 Loss2: 0.6147071123123169 Loss3: 0.6145911812782288\n",
      "alpha: [0.30653379795414504, 0.3181776126177105, 0.3752885894281446]\n",
      "9800 Total_loss 0.6089295744895935 Loss1: 0.607872486114502 Loss2: 0.6092247366905212 Loss3: 0.6095185279846191\n",
      "alpha: [0.30289776110740796, 0.31787694138661576, 0.3792252975059762]\n"
     ]
    }
   ],
   "source": [
    "alpha = [1/(len(layer_dims)-2)] * (len(layer_dims)-2)\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "            x_train = data[0]\n",
    "            y_train = data[1]\n",
    "    \n",
    "            out1, out2, out3 = model(x_train)\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            loss1 = loss_function(out1, y_train) \n",
    "            loss2 = loss_function(out2, y_train) \n",
    "            loss3 = loss_function(out3, y_train) \n",
    "            #loss4 = loss_function(out4, y_train) * alpha[3]\n",
    "            #loss5 = loss_function(out5, y_train) * alpha[4]\n",
    "            #loss6 = loss_function(out6, y_train) * alpha[5]\n",
    "            #loss7 = loss_function(out7, y_train) * alpha[6]\n",
    "            #loss8 = loss_function(out8, y_train) * alpha[7]\n",
    "\n",
    "            loss = loss1 * alpha[0] + loss2 * alpha[1] + loss3 * alpha[2] \n",
    "\n",
    "            losses.append(loss1.item());losses.append(loss2.item());losses.append(loss3.item());\n",
    "            #losses.append(loss5.item());losses.append(loss6.item());losses.append(loss7.item());losses.append(loss8.item());\n",
    "            M = sum(losses)\n",
    "            losses = [loss / M for loss in losses]\n",
    "            min_loss = np.amin(losses)\n",
    "            max_loss = np.amax(losses)\n",
    "            range_loss = max_loss - min_loss\n",
    "\n",
    "            losses = [(loss-min_loss)/range_loss for loss in losses]\n",
    "            alpha_u = [0.99 ** loss for loss in losses]\n",
    "            alpha = [a * w for a, w in zip(alpha_u, alpha)]\n",
    "\n",
    "            alpha = [ max(0.01, a) for a in alpha]\n",
    "            M = sum(alpha)\n",
    "            alpha = [a / M for a in alpha]\n",
    "\n",
    "\n",
    "            if(batch_idx % 200 == 0):\n",
    "                print(batch_idx, 'Total_loss',loss.item(), \n",
    "                      'Loss1:', loss1.item(),\n",
    "                      'Loss2:', loss2.item(),\n",
    "                      'Loss3:', loss3.item())\n",
    "                      #'Loss4:', loss4.item(),\n",
    "                      #'Loss5:', loss5.item(),\n",
    "                      #'Loss6:', loss6.item(),\n",
    "                      #'Loss7:', loss7.item(),\n",
    "                      #'Loss8:', loss8.item())\n",
    "                print('alpha:', alpha)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
